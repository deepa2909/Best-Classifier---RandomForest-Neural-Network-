{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_MIS536.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNwB6ekme6MQF53HbgbML+Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepa2909/Best-Classifier---RandomForest-Neural-Network-/blob/main/Wine%20classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GIr7RbRgbvZ"
      },
      "source": [
        "# Description of Wine Dataset and Problem\n",
        "\n",
        "* Dataset has 13 attributes and 1 predictor 'class'.\n",
        "* Size of dataset is small - (178, 14)\n",
        "* Models to Random Forest and MLP Classifier\n",
        "* Accuracy is considered at the performance measure for the business problem because it is not clear how recall and precision score is going to affect the businesss. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyRflYVf0BBX"
      },
      "source": [
        "#Import the necessary packages and library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTcddj6nz5UY"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import preprocessing \n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "import time\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoeNZKtw0r5A"
      },
      "source": [
        "#Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLuzx5BE00A5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "f5de0117-2cb5-46c7-aa98-9c4d3293f82d"
      },
      "source": [
        "data_df = pd.read_csv('https://raw.githubusercontent.com/timcsmith/MIS536-Public/master/Data/wine.csv')\n",
        "\n",
        "data_df\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Malic acid</th>\n",
              "      <th>Ash</th>\n",
              "      <th>Alcalinity of ash</th>\n",
              "      <th>Magnesium</th>\n",
              "      <th>Total phenols</th>\n",
              "      <th>Flavanoids</th>\n",
              "      <th>Nonflavanoid phenols</th>\n",
              "      <th>Proanthocyanins</th>\n",
              "      <th>Color intensity</th>\n",
              "      <th>Hue</th>\n",
              "      <th>OD280/OD315 of diluted wines</th>\n",
              "      <th>Proline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>14.23</td>\n",
              "      <td>1.71</td>\n",
              "      <td>2.43</td>\n",
              "      <td>15.6</td>\n",
              "      <td>127</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.29</td>\n",
              "      <td>5.64</td>\n",
              "      <td>1.04</td>\n",
              "      <td>3.92</td>\n",
              "      <td>1065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>13.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "      <td>11.2</td>\n",
              "      <td>100</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.28</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.05</td>\n",
              "      <td>3.40</td>\n",
              "      <td>1050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "      <td>18.6</td>\n",
              "      <td>101</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>14.37</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>16.8</td>\n",
              "      <td>113</td>\n",
              "      <td>3.85</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.24</td>\n",
              "      <td>2.18</td>\n",
              "      <td>7.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>13.24</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.87</td>\n",
              "      <td>21.0</td>\n",
              "      <td>118</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.32</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.93</td>\n",
              "      <td>735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>3</td>\n",
              "      <td>13.71</td>\n",
              "      <td>5.65</td>\n",
              "      <td>2.45</td>\n",
              "      <td>20.5</td>\n",
              "      <td>95</td>\n",
              "      <td>1.68</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.52</td>\n",
              "      <td>1.06</td>\n",
              "      <td>7.70</td>\n",
              "      <td>0.64</td>\n",
              "      <td>1.74</td>\n",
              "      <td>740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>3</td>\n",
              "      <td>13.40</td>\n",
              "      <td>3.91</td>\n",
              "      <td>2.48</td>\n",
              "      <td>23.0</td>\n",
              "      <td>102</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.41</td>\n",
              "      <td>7.30</td>\n",
              "      <td>0.70</td>\n",
              "      <td>1.56</td>\n",
              "      <td>750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>3</td>\n",
              "      <td>13.27</td>\n",
              "      <td>4.28</td>\n",
              "      <td>2.26</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.35</td>\n",
              "      <td>10.20</td>\n",
              "      <td>0.59</td>\n",
              "      <td>1.56</td>\n",
              "      <td>835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>3</td>\n",
              "      <td>13.17</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.37</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120</td>\n",
              "      <td>1.65</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.53</td>\n",
              "      <td>1.46</td>\n",
              "      <td>9.30</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.62</td>\n",
              "      <td>840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>3</td>\n",
              "      <td>14.13</td>\n",
              "      <td>4.10</td>\n",
              "      <td>2.74</td>\n",
              "      <td>24.5</td>\n",
              "      <td>96</td>\n",
              "      <td>2.05</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.35</td>\n",
              "      <td>9.20</td>\n",
              "      <td>0.61</td>\n",
              "      <td>1.60</td>\n",
              "      <td>560</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>178 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     class  Alcohol  Malic acid  ...   Hue  OD280/OD315 of diluted wines  Proline\n",
              "0        1    14.23        1.71  ...  1.04                          3.92     1065\n",
              "1        1    13.20        1.78  ...  1.05                          3.40     1050\n",
              "2        1    13.16        2.36  ...  1.03                          3.17     1185\n",
              "3        1    14.37        1.95  ...  0.86                          3.45     1480\n",
              "4        1    13.24        2.59  ...  1.04                          2.93      735\n",
              "..     ...      ...         ...  ...   ...                           ...      ...\n",
              "173      3    13.71        5.65  ...  0.64                          1.74      740\n",
              "174      3    13.40        3.91  ...  0.70                          1.56      750\n",
              "175      3    13.27        4.28  ...  0.59                          1.56      835\n",
              "176      3    13.17        2.59  ...  0.60                          1.62      840\n",
              "177      3    14.13        4.10  ...  0.61                          1.60      560\n",
              "\n",
              "[178 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woNRradf1A1N"
      },
      "source": [
        "#Quickly explore the data and fix any obvious problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgtmTIDE1Fv0"
      },
      "source": [
        "Here, I explore the number of columns, see what the columns names look like (and remove whitespace and rename when it will make it easier to work with, and check occurances of NaN (missing values)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtnqSipy1NWJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7ec690b-137b-4880-8ba3-fd00ffcb023c"
      },
      "source": [
        "data_df.columns\n",
        "data_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(178, 14)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRdNYtQI1WHZ"
      },
      "source": [
        "#Let's replace any spaces in the column names with underscore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wb0Wq0gW1cLA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "815d738a-5c7c-445c-97e2-c8b385145aba"
      },
      "source": [
        "data_df.columns = [s.strip().replace(' ','_') for s in data_df.columns] \n",
        "data_df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['class', 'Alcohol', 'Malic_acid', 'Ash', 'Alcalinity_of_ash',\n",
              "       'Magnesium', 'Total_phenols', 'Flavanoids', 'Nonflavanoid_phenols',\n",
              "       'Proanthocyanins', 'Color_intensity', 'Hue',\n",
              "       'OD280/OD315_of_diluted_wines', 'Proline'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7qaI56a1obk"
      },
      "source": [
        "#Let's check to see if there is a problem with missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj6VDi1h1s9y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c80e9e14-50f0-4ee8-94ee-83ccda64044e"
      },
      "source": [
        "data_df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "class                           0\n",
              "Alcohol                         0\n",
              "Malic_acid                      0\n",
              "Ash                             0\n",
              "Alcalinity_of_ash               0\n",
              "Magnesium                       0\n",
              "Total_phenols                   0\n",
              "Flavanoids                      0\n",
              "Nonflavanoid_phenols            0\n",
              "Proanthocyanins                 0\n",
              "Color_intensity                 0\n",
              "Hue                             0\n",
              "OD280/OD315_of_diluted_wines    0\n",
              "Proline                         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dzSrjIp7PbK"
      },
      "source": [
        "The data looks almost clean, no nulls."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlQTfG6d3Nry"
      },
      "source": [
        "#Split dataset into training (60%) and validation (40%) sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3kU_Lek3Yp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8403b06-72a7-48e6-d3e2-73aa4add27b3"
      },
      "source": [
        "# construct datasets for analysis\n",
        "target = 'class'\n",
        "predictors = [ 'Alcohol', 'Malic_acid', 'Ash', 'Alcalinity_of_ash',\n",
        "       'Magnesium', 'Total_phenols', 'Flavanoids', 'Nonflavanoid_phenols',\n",
        "       'Proanthocyanins', 'Color_intensity', 'Hue',\n",
        "       'OD280/OD315_of_diluted_wines', 'Proline']\n",
        "X = data_df[predictors]\n",
        "y = data_df[target]\n",
        "print(X)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     Alcohol  Malic_acid   Ash  ...   Hue  OD280/OD315_of_diluted_wines  Proline\n",
            "0      14.23        1.71  2.43  ...  1.04                          3.92     1065\n",
            "1      13.20        1.78  2.14  ...  1.05                          3.40     1050\n",
            "2      13.16        2.36  2.67  ...  1.03                          3.17     1185\n",
            "3      14.37        1.95  2.50  ...  0.86                          3.45     1480\n",
            "4      13.24        2.59  2.87  ...  1.04                          2.93      735\n",
            "..       ...         ...   ...  ...   ...                           ...      ...\n",
            "173    13.71        5.65  2.45  ...  0.64                          1.74      740\n",
            "174    13.40        3.91  2.48  ...  0.70                          1.56      750\n",
            "175    13.27        4.28  2.26  ...  0.59                          1.56      835\n",
            "176    13.17        2.59  2.37  ...  0.60                          1.62      840\n",
            "177    14.13        4.10  2.74  ...  0.61                          1.60      560\n",
            "\n",
            "[178 rows x 13 columns]\n",
            "0      1\n",
            "1      1\n",
            "2      1\n",
            "3      1\n",
            "4      1\n",
            "      ..\n",
            "173    3\n",
            "174    3\n",
            "175    3\n",
            "176    3\n",
            "177    3\n",
            "Name: class, Length: 178, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT2jLWvSBmhY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fd9cc8a-8a2e-46e6-a5e8-4a3f92db1b42"
      },
      "source": [
        "# create the training set and the test set \n",
        "train_X, valid_X, train_y, valid_y = train_test_split(X,y, test_size=0.4, random_state=1)\n",
        "print(train_X)\n",
        "print(valid_X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     Alcohol  Malic_acid   Ash  ...   Hue  OD280/OD315_of_diluted_wines  Proline\n",
            "168    13.58        2.58  2.69  ...  0.74                          1.80      750\n",
            "175    13.27        4.28  2.26  ...  0.59                          1.56      835\n",
            "118    12.77        3.43  1.98  ...  0.70                          2.12      372\n",
            "75     11.66        1.88  1.92  ...  1.23                          2.14      428\n",
            "21     12.93        3.80  2.65  ...  1.03                          3.52      770\n",
            "..       ...         ...   ...  ...   ...                           ...      ...\n",
            "133    12.70        3.55  2.36  ...  0.78                          1.29      600\n",
            "137    12.53        5.51  2.64  ...  0.82                          1.69      515\n",
            "72     13.49        1.66  2.24  ...  0.98                          2.78      472\n",
            "140    12.93        2.81  2.70  ...  0.77                          2.31      600\n",
            "37     13.05        1.65  2.55  ...  1.12                          2.51     1105\n",
            "\n",
            "[106 rows x 13 columns]\n",
            "     Alcohol  Malic_acid   Ash  ...   Hue  OD280/OD315_of_diluted_wines  Proline\n",
            "161    13.69        3.26  2.54  ...  0.96                          1.82      680\n",
            "117    12.42        1.61  2.19  ...  1.06                          2.96      345\n",
            "19     13.64        3.10  2.56  ...  0.96                          3.36      845\n",
            "69     12.21        1.19  1.75  ...  1.28                          3.07      718\n",
            "53     13.77        1.90  2.68  ...  1.13                          2.93     1375\n",
            "..       ...         ...   ...  ...   ...                           ...      ...\n",
            "45     14.21        4.04  2.44  ...  0.87                          3.33     1080\n",
            "93     12.29        2.83  2.22  ...  1.15                          3.30      290\n",
            "36     13.28        1.64  2.84  ...  1.09                          2.78      880\n",
            "171    12.77        2.39  2.28  ...  0.57                          1.63      470\n",
            "166    13.45        3.70  2.60  ...  0.85                          1.56      695\n",
            "\n",
            "[72 rows x 13 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEP0onc5BzDr"
      },
      "source": [
        "#Create an initial 'wide' range of possible hyperparameter values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AM0QLC8CazT"
      },
      "source": [
        "Here we create a wide range of possible parameter values for each of the hyperparameters for this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOv6ELRbCnPX"
      },
      "source": [
        "\n",
        "# Number of trees in random forest; default is 100\n",
        "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
        "\n",
        "# Criterion used to guide data splits\n",
        "criterion = ['gini', 'entropy']\n",
        "\n",
        "# Maximum number of levels in tree. If None, then nodes are expanded until all leaves are pure or until all \n",
        "# leaves contain less than min_samples_split samples.\n",
        "# default = None\n",
        "max_depth = [int(x) for x in np.linspace(8, 120, num = 11)]\n",
        "max_depth.append(None)\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "# default is 2\n",
        "min_samples_split = [2, 5, 13]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "# default = 1 \n",
        "min_samples_leaf = [1, 2, 5]\n",
        "\n",
        "# Number of features to consider at every split\n",
        "# default is auto (which is equivalent to sqrt)\n",
        "max_features = ['auto']\n",
        "\n",
        "# max_leaf_nodes  - Grow trees with max_leaf_nodes in best-first fashion.\n",
        "# If None then unlimited number of leaf nodes.\n",
        "# default=None \n",
        "max_leaf_nodes = [None]\n",
        "\n",
        "# min_impurity_decrease - A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
        "# default=0.0\n",
        "min_impurity_decrease = [0.001, 0.005, 0.01, 0.05, 0.02]\n",
        "\n",
        "# Method of selecting samples for training each tree\n",
        "# default = True,  If False, the whole dataset is used to build each tree.\n",
        "bootstrap = [True]\n",
        "\n",
        "# Create the random grid\n",
        "param_grid_random = {'n_estimators': n_estimators,\n",
        "                      'criterion': criterion,\n",
        "                      'max_depth': max_depth,\n",
        "                      'min_samples_split': min_samples_split,\n",
        "                      'min_samples_leaf' : min_samples_leaf,\n",
        "                      'max_features': max_features,\n",
        "                      'max_leaf_nodes' : max_leaf_nodes,\n",
        "                      'min_impurity_decrease' : min_impurity_decrease,\n",
        "                      'bootstrap': bootstrap,\n",
        "                     }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IALeSYulDHrH"
      },
      "source": [
        " # Use Randomize Search to narrow the possible range of parameter values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OkswNaODMMF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "278e3099-724a-4621-d41b-c2722d3d56e1"
      },
      "source": [
        "# Use the param_grid_random for an initial \"rough\" search using Randomized search\n",
        "rand_f = RandomForestClassifier()\n",
        "\n",
        "randomSearch = RandomizedSearchCV(estimator = rand_f, param_distributions = param_grid_random, n_iter = 250, cv = 3, verbose=2, random_state=24, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "randomSearch.fit(train_X, train_y)\n",
        "bestRandomModel = randomSearch.best_estimator_\n",
        "print('Best parameters found: ', randomSearch.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   28.8s\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:  1.7min\n",
            "[Parallel(n_jobs=-1)]: Done 361 tasks      | elapsed:  4.0min\n",
            "[Parallel(n_jobs=-1)]: Done 644 tasks      | elapsed:  7.2min\n",
            "[Parallel(n_jobs=-1)]: Done 750 out of 750 | elapsed:  8.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best parameters found:  {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.005, 'max_leaf_nodes': None, 'max_features': 'auto', 'max_depth': None, 'criterion': 'entropy', 'bootstrap': True}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87j-oY9FDh5A"
      },
      "source": [
        "#Test the performance of the selected parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb3WBca1Dm0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98821b14-0139-417e-8ec8-cd377cb29e35"
      },
      "source": [
        "validation_predictions = bestRandomModel.predict(valid_X)\n",
        "print('Accuracy Score: ', metrics.accuracy_score(valid_y, validation_predictions))\n",
        "print('Precision Score: ', metrics.precision_score(valid_y, validation_predictions, average ='weighted'))\n",
        "print('Recall Score: ', metrics.recall_score(valid_y, validation_predictions, average ='weighted'))\n",
        "print('Confusion Matrix: \\n ', metrics.confusion_matrix(valid_y, validation_predictions))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score:  0.9861111111111112\n",
            "Precision Score:  0.9865900383141764\n",
            "Recall Score:  0.9861111111111112\n",
            "Confusion Matrix: \n",
            "  [[28  0  0]\n",
            " [ 1 26  0]\n",
            " [ 0  0 17]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRJF64bqD6NS"
      },
      "source": [
        "#Use knowledge gained from above two steps to create new 'narrow' range of possible hyperparameter values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46AMo18MEV9N"
      },
      "source": [
        "# let's take the best parameters from the the random search, and use this as a base for gridsearch\n",
        "param_grid = {'n_estimators': [170, 180, 200, 210, 220],\n",
        "              'min_samples_split': [2, 3, 5, 7],  \n",
        "              'min_samples_leaf': [1, 2],\n",
        "              'min_impurity_decrease': [0.000, 0.005, 0.001, 0.002],\n",
        "              'max_leaf_nodes': [None], \n",
        "              'max_features': ['auto'], \n",
        "              'criterion': ['entropy'],\n",
        "              'bootstrap': [True]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqjpbtAnEZzK"
      },
      "source": [
        " # Use Grid (exhaustive) to refine model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRn5xwGKEe0B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17b640cd-7ac1-42c3-86e3-aee77207478e"
      },
      "source": [
        "# refine our search using param_grid\n",
        "rand_f2 = RandomForestClassifier()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "gridSearch = GridSearchCV(estimator = rand_f2, param_grid = param_grid,  cv = 2, verbose=2,  n_jobs = -1)\n",
        "# Fit the exhaustive search model\n",
        "gridSearch.fit(train_X, train_y)\n",
        "bestGridModel = gridSearch.best_estimator_\n",
        "print('Best parameters found: ', gridSearch.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 160 candidates, totalling 320 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:    8.9s\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:   37.5s\n",
            "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:  1.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best parameters found:  {'bootstrap': True, 'criterion': 'entropy', 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 210}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-dDc0AROPOU"
      },
      "source": [
        "# Performance of the model using identified parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcFJc5gKN8xf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18ee2bb9-c042-4423-a290-8df87690c8f7"
      },
      "source": [
        "validation_predictions = bestGridModel.predict(valid_X)\n",
        "print('Accuracy Score: ', metrics.accuracy_score(valid_y, validation_predictions))\n",
        "print('Precision Score: ', metrics.precision_score(valid_y, validation_predictions, average ='weighted'))\n",
        "print('Recall Score: ', metrics.recall_score(valid_y, validation_predictions, average ='weighted'))\n",
        "print('Confusion Matrix: \\n ', metrics.confusion_matrix(valid_y, validation_predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score:  0.9861111111111112\n",
            "Precision Score:  0.9865900383141764\n",
            "Recall Score:  0.9861111111111112\n",
            "Confusion Matrix: \n",
            "  [[28  0  0]\n",
            " [ 1 26  0]\n",
            " [ 0  0 17]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj_MggmDOTk7"
      },
      "source": [
        "Since, after performing randomized grid search and exhaustive grid search for hypertuning parameter to find best parameter for random forest model, after using both, the performance of random forest model didn't improve. Therefore, I am going to stop hyperparameter tuning here and model a neural network classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF84lS_uO06Q"
      },
      "source": [
        "# Neural Network (testing at least a 1-layer, a 2-layer, and a 3-layer model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOKMjEb3QG4F"
      },
      "source": [
        "# Fit MLPClassifier (Multi-Layer Perceptron Classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2YfHTg8QMTW"
      },
      "source": [
        "* Layer **1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZYpUzBsQWZc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b6f5f16-d007-48c7-ae63-330c6817361c"
      },
      "source": [
        "#combination 1\n",
        "%%time\n",
        "start = time.time()\n",
        "\n",
        "model = MLPClassifier(hidden_layer_sizes=(500), solver='adam', max_iter=200, verbose=True)\n",
        "model.fit(train_X, train_y)\n",
        "\n",
        "end = time.time()\n",
        "print(\"Total Time\", end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 27.18164751\n",
            "Iteration 2, loss = 31.80458214\n",
            "Iteration 3, loss = 22.90783404\n",
            "Iteration 4, loss = 9.34850699\n",
            "Iteration 5, loss = 8.93523795\n",
            "Iteration 6, loss = 11.45633320\n",
            "Iteration 7, loss = 13.52499487\n",
            "Iteration 8, loss = 11.71919024\n",
            "Iteration 9, loss = 12.10607874\n",
            "Iteration 10, loss = 9.70387311\n",
            "Iteration 11, loss = 9.29349660\n",
            "Iteration 12, loss = 6.04319036\n",
            "Iteration 13, loss = 2.93023602\n",
            "Iteration 14, loss = 2.98738302\n",
            "Iteration 15, loss = 5.59564225\n",
            "Iteration 16, loss = 4.35205516\n",
            "Iteration 17, loss = 4.00805677\n",
            "Iteration 18, loss = 4.37322189\n",
            "Iteration 19, loss = 2.82970261\n",
            "Iteration 20, loss = 0.91691556\n",
            "Iteration 21, loss = 3.06049700\n",
            "Iteration 22, loss = 2.43984060\n",
            "Iteration 23, loss = 3.03621676\n",
            "Iteration 24, loss = 1.67305430\n",
            "Iteration 25, loss = 1.44117045\n",
            "Iteration 26, loss = 2.27509120\n",
            "Iteration 27, loss = 2.10757029\n",
            "Iteration 28, loss = 1.80738349\n",
            "Iteration 29, loss = 1.93764862\n",
            "Iteration 30, loss = 0.90927767\n",
            "Iteration 31, loss = 0.71584948\n",
            "Iteration 32, loss = 1.60797467\n",
            "Iteration 33, loss = 1.57259612\n",
            "Iteration 34, loss = 0.90303906\n",
            "Iteration 35, loss = 0.55182705\n",
            "Iteration 36, loss = 1.05974459\n",
            "Iteration 37, loss = 1.28084734\n",
            "Iteration 38, loss = 0.89693556\n",
            "Iteration 39, loss = 0.60553893\n",
            "Iteration 40, loss = 0.76105049\n",
            "Iteration 41, loss = 0.90129128\n",
            "Iteration 42, loss = 0.66991943\n",
            "Iteration 43, loss = 0.56640663\n",
            "Iteration 44, loss = 0.82865925\n",
            "Iteration 45, loss = 0.54548795\n",
            "Iteration 46, loss = 0.47478402\n",
            "Iteration 47, loss = 0.67361006\n",
            "Iteration 48, loss = 0.66251319\n",
            "Iteration 49, loss = 0.46778068\n",
            "Iteration 50, loss = 0.47157724\n",
            "Iteration 51, loss = 0.56844753\n",
            "Iteration 52, loss = 0.46590947\n",
            "Iteration 53, loss = 0.48611596\n",
            "Iteration 54, loss = 0.52139881\n",
            "Iteration 55, loss = 0.41466343\n",
            "Iteration 56, loss = 0.41822147\n",
            "Iteration 57, loss = 0.48518155\n",
            "Iteration 58, loss = 0.44967761\n",
            "Iteration 59, loss = 0.40487455\n",
            "Iteration 60, loss = 0.42484814\n",
            "Iteration 61, loss = 0.41273903\n",
            "Iteration 62, loss = 0.38361357\n",
            "Iteration 63, loss = 0.42009918\n",
            "Iteration 64, loss = 0.40742824\n",
            "Iteration 65, loss = 0.36889821\n",
            "Iteration 66, loss = 0.38489277\n",
            "Iteration 67, loss = 0.38409601\n",
            "Iteration 68, loss = 0.36912607\n",
            "Iteration 69, loss = 0.37955872\n",
            "Iteration 70, loss = 0.37177815\n",
            "Iteration 71, loss = 0.35098582\n",
            "Iteration 72, loss = 0.36446662\n",
            "Iteration 73, loss = 0.36515201\n",
            "Iteration 74, loss = 0.34972365\n",
            "Iteration 75, loss = 0.35548515\n",
            "Iteration 76, loss = 0.34962921\n",
            "Iteration 77, loss = 0.34233091\n",
            "Iteration 78, loss = 0.35016993\n",
            "Iteration 79, loss = 0.34490147\n",
            "Iteration 80, loss = 0.33577689\n",
            "Iteration 81, loss = 0.33990385\n",
            "Iteration 82, loss = 0.33709122\n",
            "Iteration 83, loss = 0.33310067\n",
            "Iteration 84, loss = 0.33490804\n",
            "Iteration 85, loss = 0.32908966\n",
            "Iteration 86, loss = 0.32658348\n",
            "Iteration 87, loss = 0.32895258\n",
            "Iteration 88, loss = 0.32489786\n",
            "Iteration 89, loss = 0.32194064\n",
            "Iteration 90, loss = 0.32174940\n",
            "Iteration 91, loss = 0.31850107\n",
            "Iteration 92, loss = 0.31750156\n",
            "Iteration 93, loss = 0.31696436\n",
            "Iteration 94, loss = 0.31291075\n",
            "Iteration 95, loss = 0.31192927\n",
            "Iteration 96, loss = 0.31104929\n",
            "Iteration 97, loss = 0.30857256\n",
            "Iteration 98, loss = 0.30755140\n",
            "Iteration 99, loss = 0.30507773\n",
            "Iteration 100, loss = 0.30382599\n",
            "Iteration 101, loss = 0.30298752\n",
            "Iteration 102, loss = 0.30065052\n",
            "Iteration 103, loss = 0.29943340\n",
            "Iteration 104, loss = 0.29793639\n",
            "Iteration 105, loss = 0.29651833\n",
            "Iteration 106, loss = 0.29534577\n",
            "Iteration 107, loss = 0.29356848\n",
            "Iteration 108, loss = 0.29218430\n",
            "Iteration 109, loss = 0.29091956\n",
            "Iteration 110, loss = 0.28955729\n",
            "Iteration 111, loss = 0.28795474\n",
            "Iteration 112, loss = 0.28650967\n",
            "Iteration 113, loss = 0.28543064\n",
            "Iteration 114, loss = 0.28413607\n",
            "Iteration 115, loss = 0.28240169\n",
            "Iteration 116, loss = 0.28046920\n",
            "Iteration 117, loss = 0.27923054\n",
            "Iteration 118, loss = 0.27767910\n",
            "Iteration 119, loss = 0.27643535\n",
            "Iteration 120, loss = 0.27493877\n",
            "Iteration 121, loss = 0.27335762\n",
            "Iteration 122, loss = 0.27223333\n",
            "Iteration 123, loss = 0.27068545\n",
            "Iteration 124, loss = 0.26894958\n",
            "Iteration 125, loss = 0.26722052\n",
            "Iteration 126, loss = 0.26565629\n",
            "Iteration 127, loss = 0.26398363\n",
            "Iteration 128, loss = 0.26240898\n",
            "Iteration 129, loss = 0.26079448\n",
            "Iteration 130, loss = 0.25891095\n",
            "Iteration 131, loss = 0.25710371\n",
            "Iteration 132, loss = 0.25526427\n",
            "Iteration 133, loss = 0.25372069\n",
            "Iteration 134, loss = 0.25213038\n",
            "Iteration 135, loss = 0.25049332\n",
            "Iteration 136, loss = 0.24881635\n",
            "Iteration 137, loss = 0.24715045\n",
            "Iteration 138, loss = 0.24549971\n",
            "Iteration 139, loss = 0.24393614\n",
            "Iteration 140, loss = 0.24243488\n",
            "Iteration 141, loss = 0.24082076\n",
            "Iteration 142, loss = 0.23931873\n",
            "Iteration 143, loss = 0.23790997\n",
            "Iteration 144, loss = 0.23637213\n",
            "Iteration 145, loss = 0.23485850\n",
            "Iteration 146, loss = 0.23340138\n",
            "Iteration 147, loss = 0.23194680\n",
            "Iteration 148, loss = 0.23050862\n",
            "Iteration 149, loss = 0.22901798\n",
            "Iteration 150, loss = 0.22775007\n",
            "Iteration 151, loss = 0.22617203\n",
            "Iteration 152, loss = 0.22493082\n",
            "Iteration 153, loss = 0.22360216\n",
            "Iteration 154, loss = 0.22214043\n",
            "Iteration 155, loss = 0.22076347\n",
            "Iteration 156, loss = 0.21951638\n",
            "Iteration 157, loss = 0.21801015\n",
            "Iteration 158, loss = 0.21709873\n",
            "Iteration 159, loss = 0.21567226\n",
            "Iteration 160, loss = 0.21460455\n",
            "Iteration 161, loss = 0.21344691\n",
            "Iteration 162, loss = 0.21194950\n",
            "Iteration 163, loss = 0.21061402\n",
            "Iteration 164, loss = 0.21021496\n",
            "Iteration 165, loss = 0.20856795\n",
            "Iteration 166, loss = 0.20715346\n",
            "Iteration 167, loss = 0.20672212\n",
            "Iteration 168, loss = 0.20477699\n",
            "Iteration 169, loss = 0.20386965\n",
            "Iteration 170, loss = 0.20276344\n",
            "Iteration 171, loss = 0.20143264\n",
            "Iteration 172, loss = 0.20070818\n",
            "Iteration 173, loss = 0.19909146\n",
            "Iteration 174, loss = 0.19813007\n",
            "Iteration 175, loss = 0.19688441\n",
            "Iteration 176, loss = 0.19579620\n",
            "Iteration 177, loss = 0.19498075\n",
            "Iteration 178, loss = 0.19353920\n",
            "Iteration 179, loss = 0.19277596\n",
            "Iteration 180, loss = 0.19164815\n",
            "Iteration 181, loss = 0.19050592\n",
            "Iteration 182, loss = 0.18965764\n",
            "Iteration 183, loss = 0.18835598\n",
            "Iteration 184, loss = 0.18745135\n",
            "Iteration 185, loss = 0.18596677\n",
            "Iteration 186, loss = 0.18451891\n",
            "Iteration 187, loss = 0.18352885\n",
            "Iteration 188, loss = 0.18179258\n",
            "Iteration 189, loss = 0.18096281\n",
            "Iteration 190, loss = 0.18003896\n",
            "Iteration 191, loss = 0.17848821\n",
            "Iteration 192, loss = 0.17797227\n",
            "Iteration 193, loss = 0.17674278\n",
            "Iteration 194, loss = 0.17562018\n",
            "Iteration 195, loss = 0.17504201\n",
            "Iteration 196, loss = 0.17371849\n",
            "Iteration 197, loss = 0.17303144\n",
            "Iteration 198, loss = 0.17228654\n",
            "Iteration 199, loss = 0.17134189\n",
            "Iteration 200, loss = 0.17075275\n",
            "Total Time 0.6598198413848877\n",
            "CPU times: user 696 ms, sys: 479 ms, total: 1.17 s\n",
            "Wall time: 660 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkhIqcttRPEZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc944002-0ed0-41c1-c43d-39e11d235cbf"
      },
      "source": [
        "%%time\n",
        "start = time.time()\n",
        "\n",
        "sample_X = valid_X.iloc[1]\n",
        "sample_X = sample_X.values.reshape(1, -1)\n",
        "print(sample_X)\n",
        "print(model.predict(sample_X))\n",
        "\n",
        "end = time.time()\n",
        "print(\"Total Time\", end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.242e+01 1.610e+00 2.190e+00 2.250e+01 1.080e+02 2.000e+00 2.090e+00\n",
            "  3.400e-01 1.610e+00 2.060e+00 1.060e+00 2.960e+00 3.450e+02]]\n",
            "[2]\n",
            "Total Time 0.0037384033203125\n",
            "CPU times: user 4.13 ms, sys: 5 µs, total: 4.14 ms\n",
            "Wall time: 3.83 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fkFnaeWTknf"
      },
      "source": [
        "# Test the performance of the selected parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUI8r7T6RUDA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1eeb6de-265d-4282-de66-9b0ba72fa49d"
      },
      "source": [
        "%%time\n",
        "start = time.time()\n",
        "validation_predictions = model.predict(valid_X)\n",
        "end = time.time()\n",
        "print(\"Total Time per prediction =\", (end - start)/len(valid_X))\n",
        "print('confusion_matrix:\\n ', confusion_matrix(valid_y, validation_predictions))\n",
        "print('Accuracy Score: ', metrics.accuracy_score(valid_y, validation_predictions))\n",
        "print('Precision Score: ', metrics.precision_score(valid_y, validation_predictions, average ='weighted'))\n",
        "print('Recall Score: ', metrics.recall_score(valid_y, validation_predictions, average ='weighted'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Time per prediction = 6.091263559129503e-05\n",
            "confusion_matrix:\n",
            "  [[27  1  0]\n",
            " [ 0 27  0]\n",
            " [ 0  2 15]]\n",
            "Accuracy Score:  0.9583333333333334\n",
            "Precision Score:  0.9624999999999999\n",
            "Recall Score:  0.9583333333333334\n",
            "CPU times: user 13.6 ms, sys: 2.01 ms, total: 15.6 ms\n",
            "Wall time: 12.7 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXkra5vmSAAM"
      },
      "source": [
        "* Layer 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFpeGVRHR9FU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d09353e-9834-42ec-bf7b-c37c9285b7b4"
      },
      "source": [
        "#combination 2 \n",
        "%%time\n",
        "start = time.time()\n",
        "\n",
        "model2 = MLPClassifier(hidden_layer_sizes=(500, 250), solver='adam', max_iter=200, verbose=True)\n",
        "model2.fit(train_X, train_y)\n",
        "\n",
        "end = time.time()\n",
        "print(\"Total Time\", end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 9.67171205\n",
            "Iteration 2, loss = 52.53655669\n",
            "Iteration 3, loss = 85.87874096\n",
            "Iteration 4, loss = 70.43160295\n",
            "Iteration 5, loss = 35.40925870\n",
            "Iteration 6, loss = 20.78792628\n",
            "Iteration 7, loss = 17.57679717\n",
            "Iteration 8, loss = 28.89047312\n",
            "Iteration 9, loss = 29.56181524\n",
            "Iteration 10, loss = 25.26194169\n",
            "Iteration 11, loss = 30.19915918\n",
            "Iteration 12, loss = 22.83668427\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Total Time 0.15098047256469727\n",
            "CPU times: user 180 ms, sys: 82.2 ms, total: 262 ms\n",
            "Wall time: 151 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAZZHU8iSN1t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8f235e2-9769-42a0-ebde-610f9fafe794"
      },
      "source": [
        "%%time\n",
        "start = time.time()\n",
        "\n",
        "sample_X = valid_X.iloc[1]\n",
        "sample_X = sample_X.values.reshape(1, -1)\n",
        "print(sample_X)\n",
        "print(model.predict(sample_X))\n",
        "\n",
        "end = time.time()\n",
        "print(\"Total Time\", end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.242e+01 1.610e+00 2.190e+00 2.250e+01 1.080e+02 2.000e+00 2.090e+00\n",
            "  3.400e-01 1.610e+00 2.060e+00 1.060e+00 2.960e+00 3.450e+02]]\n",
            "[3]\n",
            "Total Time 0.01052403450012207\n",
            "CPU times: user 5.41 ms, sys: 5.12 ms, total: 10.5 ms\n",
            "Wall time: 10.7 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_QosxfcTnY8"
      },
      "source": [
        "# Test the performance of the selected parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgFwsO9RSTf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "301ea757-e292-4da5-b0da-eb5d8307b2fe"
      },
      "source": [
        "%%time\n",
        "start = time.time()\n",
        "validation_predictions = model2.predict(valid_X)\n",
        "end = time.time()\n",
        "print(\"Total Time per prediction =\", (end - start)/len(valid_X))\n",
        "print('confusion_matrix:\\n ', confusion_matrix(valid_y, validation_predictions))\n",
        "print('Accuracy Score: ', metrics.accuracy_score(valid_y, validation_predictions))\n",
        "print('Precision Score: ', metrics.precision_score(valid_y, validation_predictions, average ='weighted'))\n",
        "print('Recall Score: ', metrics.recall_score(valid_y, validation_predictions, average ='weighted'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Time per prediction = 0.0001948972543080648\n",
            "confusion_matrix:\n",
            "  [[28  0  0]\n",
            " [17 10  0]\n",
            " [15  2  0]]\n",
            "Accuracy Score:  0.5277777777777778\n",
            "Precision Score:  0.4939814814814814\n",
            "Recall Score:  0.5277777777777778\n",
            "CPU times: user 14.8 ms, sys: 9.26 ms, total: 24.1 ms\n",
            "Wall time: 26.1 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHUVUNDJShPC"
      },
      "source": [
        "* Layer -3 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3pzYIwcSkJz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b67e865-d844-45e7-f103-266046155c62"
      },
      "source": [
        "#combination 3 \n",
        "%%time\n",
        "start = time.time()\n",
        "\n",
        "model3 = MLPClassifier(hidden_layer_sizes=(30, 50, 75), solver='adam', max_iter=200, verbose=True)\n",
        "model3.fit(train_X, train_y)\n",
        "\n",
        "end = time.time()\n",
        "print(\"Total Time\", end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 51.62472700\n",
            "Iteration 2, loss = 40.60244922\n",
            "Iteration 3, loss = 35.08389358\n",
            "Iteration 4, loss = 27.20211813\n",
            "Iteration 5, loss = 24.38486320\n",
            "Iteration 6, loss = 18.52900915\n",
            "Iteration 7, loss = 12.42475267\n",
            "Iteration 8, loss = 8.92262055\n",
            "Iteration 9, loss = 5.94895009\n",
            "Iteration 10, loss = 12.21553959\n",
            "Iteration 11, loss = 12.41050907\n",
            "Iteration 12, loss = 8.38865275\n",
            "Iteration 13, loss = 6.62582523\n",
            "Iteration 14, loss = 7.22136306\n",
            "Iteration 15, loss = 5.14765131\n",
            "Iteration 16, loss = 4.57836204\n",
            "Iteration 17, loss = 6.05699979\n",
            "Iteration 18, loss = 3.18943434\n",
            "Iteration 19, loss = 3.39479086\n",
            "Iteration 20, loss = 3.83756658\n",
            "Iteration 21, loss = 3.22484965\n",
            "Iteration 22, loss = 3.02838375\n",
            "Iteration 23, loss = 3.18170157\n",
            "Iteration 24, loss = 1.72145301\n",
            "Iteration 25, loss = 2.68705062\n",
            "Iteration 26, loss = 2.11719221\n",
            "Iteration 27, loss = 1.57866503\n",
            "Iteration 28, loss = 2.36816531\n",
            "Iteration 29, loss = 2.66339037\n",
            "Iteration 30, loss = 1.99575325\n",
            "Iteration 31, loss = 1.13172635\n",
            "Iteration 32, loss = 0.85961346\n",
            "Iteration 33, loss = 1.62464895\n",
            "Iteration 34, loss = 1.83912799\n",
            "Iteration 35, loss = 1.29989000\n",
            "Iteration 36, loss = 0.91944355\n",
            "Iteration 37, loss = 0.86090405\n",
            "Iteration 38, loss = 1.20776974\n",
            "Iteration 39, loss = 1.42551768\n",
            "Iteration 40, loss = 1.28103382\n",
            "Iteration 41, loss = 0.94603594\n",
            "Iteration 42, loss = 0.74566055\n",
            "Iteration 43, loss = 0.94403870\n",
            "Iteration 44, loss = 1.10635763\n",
            "Iteration 45, loss = 0.99878638\n",
            "Iteration 46, loss = 0.96153475\n",
            "Iteration 47, loss = 0.79181409\n",
            "Iteration 48, loss = 0.76945884\n",
            "Iteration 49, loss = 0.91724918\n",
            "Iteration 50, loss = 0.95414217\n",
            "Iteration 51, loss = 0.85280208\n",
            "Iteration 52, loss = 0.78873220\n",
            "Iteration 53, loss = 0.78331523\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Total Time 0.15584254264831543\n",
            "CPU times: user 165 ms, sys: 104 ms, total: 268 ms\n",
            "Wall time: 156 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02aO_gjMSxS3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "019b6a50-b73f-425c-9f6d-6d283cf82496"
      },
      "source": [
        "%%time\n",
        "start = time.time()\n",
        "\n",
        "sample_X = valid_X.iloc[1]\n",
        "sample_X = sample_X.values.reshape(1, -1)\n",
        "print(sample_X)\n",
        "print(model.predict(sample_X))\n",
        "\n",
        "end = time.time()\n",
        "print(\"Total Time\", end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.242e+01 1.610e+00 2.190e+00 2.250e+01 1.080e+02 2.000e+00 2.090e+00\n",
            "  3.400e-01 1.610e+00 2.060e+00 1.060e+00 2.960e+00 3.450e+02]]\n",
            "[3]\n",
            "Total Time 0.002755880355834961\n",
            "CPU times: user 3.11 ms, sys: 1.03 ms, total: 4.14 ms\n",
            "Wall time: 2.83 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPntFPYiTqW9"
      },
      "source": [
        "# Test the performance of the selected parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdS4Gr3AS2qS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ce959eb-57e9-4df0-8e2c-46c52d14051e"
      },
      "source": [
        "%%time\n",
        "start = time.time()\n",
        "validation_predictions = model3.predict(valid_X)\n",
        "end = time.time()\n",
        "print(\"Total Time per prediction =\", (end - start)/len(valid_X))\n",
        "print('confusion_matrix:\\n ', confusion_matrix(valid_y, validation_predictions))\n",
        "print('Accuracy Score: ', metrics.accuracy_score(valid_y, validation_predictions))\n",
        "print('Precision Score: ', metrics.precision_score(valid_y, validation_predictions, average ='weighted'))\n",
        "print('Recall Score: ', metrics.recall_score(valid_y, validation_predictions, average ='weighted'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Time per prediction = 0.00012489491038852267\n",
            "confusion_matrix:\n",
            "  [[20  7  1]\n",
            " [ 0 22  5]\n",
            " [ 0  6 11]]\n",
            "Accuracy Score:  0.7361111111111112\n",
            "Precision Score:  0.7773809523809524\n",
            "Recall Score:  0.7361111111111112\n",
            "CPU times: user 10 ms, sys: 8.15 ms, total: 18.2 ms\n",
            "Wall time: 16.4 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIpWE8cJbVO4"
      },
      "source": [
        "# Condition 4 - GridSearchCV Hyperparameter tunning "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXhCjTQ8bXS-"
      },
      "source": [
        "\n",
        "param_grid2 = {\n",
        "'hidden_layer_sizes': [(110,50,20), (260,200,140, 120), (360,325,300)], \n",
        "'activation': ['tanh'],\n",
        "'solver': ['adam'],\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88dj9Ps8bbwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52e22f58-b417-4157-bf71-423047f5595b"
      },
      "source": [
        "#combination 4\n",
        "\n",
        "%%time\n",
        "start = time.time()\n",
        "\n",
        "model = MLPClassifier()\n",
        "gridSearch = GridSearchCV(estimator = model, param_grid = param_grid2,  cv = 3, verbose=2, n_jobs = -1)\n",
        "gridSearch.fit(train_X, train_y)\n",
        "\n",
        "bestgridmodel_mlp_1 = gridSearch.best_estimator_\n",
        "print('Best parameters found: ', gridSearch.best_params_)\n",
        "\n",
        "end = time.time()\n",
        "print(\"Total Time\", end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:   10.7s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best parameters found:  {'activation': 'tanh', 'hidden_layer_sizes': (360, 325, 300), 'solver': 'adam'}\n",
            "Total Time 13.401542901992798\n",
            "CPU times: user 3.65 s, sys: 1.76 s, total: 5.42 s\n",
            "Wall time: 13.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHl-4gmVbev4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c11cabf-0be1-4b06-8979-87a2cf04439e"
      },
      "source": [
        "%%time\n",
        "start = time.time()\n",
        "\n",
        "sample_X = valid_X.iloc[1]\n",
        "sample_X = sample_X.values.reshape(1, -1)\n",
        "print(sample_X)\n",
        "print(gridSearch.predict(sample_X))\n",
        "\n",
        "end = time.time()\n",
        "print(\"Total Time\", end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.242e+01 1.610e+00 2.190e+00 2.250e+01 1.080e+02 2.000e+00 2.090e+00\n",
            "  3.400e-01 1.610e+00 2.060e+00 1.060e+00 2.960e+00 3.450e+02]]\n",
            "[2]\n",
            "Total Time 0.00531315803527832\n",
            "CPU times: user 4.94 ms, sys: 1.81 ms, total: 6.74 ms\n",
            "Wall time: 5.44 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aA-tsooePR9"
      },
      "source": [
        "# Performance of measures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QvxTl9ZbhsV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0775301-2672-4fa5-a66a-8ca57bfff5aa"
      },
      "source": [
        "\n",
        "print('confusion_matrix:\\n ', confusion_matrix(valid_y, validation_predictions))\n",
        "print('Accuracy Score: ', metrics.accuracy_score(valid_y, validation_predictions))\n",
        "print('Precision Score: ', metrics.precision_score(valid_y, validation_predictions, average ='weighted'))\n",
        "print('Recall Score: ', metrics.recall_score(valid_y, validation_predictions, average ='weighted'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confusion_matrix:\n",
            "  [[20  7  1]\n",
            " [ 0 22  5]\n",
            " [ 0  6 11]]\n",
            "Accuracy Score:  0.7361111111111112\n",
            "Precision Score:  0.7773809523809524\n",
            "Recall Score:  0.7361111111111112\n",
            "CPU times: user 8.51 ms, sys: 1.76 ms, total: 10.3 ms\n",
            "Wall time: 11.8 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U99pvNUAT1iv"
      },
      "source": [
        "#Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQDdHOZUT78k"
      },
      "source": [
        "* # Random Forest with RandomizedSearched hyperparameter tuning of parameters\n",
        "\n",
        "Test performance measure after randomized search for paramters:\n",
        "\n",
        "    Accuracy Score:  0.9861111111111112\n",
        "    Precision Score:  0.9865900383141764\n",
        "    Recall Score:  0.9861111111111112\n",
        "    Confusion Matrix: \n",
        "      [[28  0  0]\n",
        "      [ 1 26  0]\n",
        "      [ 0  0 17]]\n",
        "\n",
        "The randomized searched parameters seems to be giving fine performance for the random forest model. After, looking at the confusion matrix, it can be said that model predicts 1 category wrong and rest predicts accurate. So, if hyperparameter tuning of paramters gives better performance of the model than this then only it should be selected. \n",
        "\n",
        " # Random Forest with GridSearched hyperparameter tuning of parameters \n",
        "\n",
        "On GridSearch, the best parameters come out to be:\n",
        "\n",
        "\n",
        "Best parameters found:  {'bootstrap': True, 'criterion': 'entropy', 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 210}\n",
        "\n",
        "The performance of the selected parameters:\n",
        "\n",
        "    Accuracy Score:  0.9861111111111112\n",
        "    Precision Score:  0.9865900383141764\n",
        "    Recall Score:  0.9861111111111112\n",
        "    Confusion Matrix: \n",
        "      [[28  0  0]\n",
        "      [ 1 26  0]\n",
        "      [ 0  0 17]]\n",
        "\n",
        "The performance of random forest model before and after exhaustive hyperparameter tuning didn't change, therefore, these parameters give an accuracy of 98.61 % in both stages. Since, the busines context of developing this model is not clear, for example, how the high precision score is going to benefit or how recall score is going to affect the business is not clear, therfore, in this condition, I am going to select the accuracy measure the performance measure for this business model. And, I would recommend that model which will show a higher accuracy.\n",
        "\n",
        "# MLP Classifier \n",
        "\n",
        "* Condition 1, Layer - 1\n",
        "    Total Time per prediction = 6.091263559129503e-05\n",
        "    confusion_matrix:\n",
        "      [[27  1  0]\n",
        "      [ 0 27  0]\n",
        "      [ 0  2 15]]\n",
        "    Accuracy Score:  0.9583333333333334\n",
        "    Precision Score:  0.9624999999999999\n",
        "    Recall Score:  0.9583333333333334\n",
        "    \n",
        "* Condition 2, Layer - 2\n",
        "  \n",
        "    confusion_matrix:\n",
        "      [[28  0  0]\n",
        "      [17 10  0]\n",
        "      [15  2  0]]\n",
        "    Accuracy Score:  0.5277777777777778\n",
        "    Precision Score:  0.4939814814814814\n",
        "    Recall Score:  0.5277777777777778\n",
        "\n",
        "* Condition 3, Layer - 3\n",
        "    confusion_matrix:\n",
        "        [[20  7  1]\n",
        "        [ 0 22  5]\n",
        "        [ 0  6 11]]\n",
        "    Accuracy Score:  0.7361111111111112\n",
        "    Precision Score:  0.7773809523809524\n",
        "    Recall Score:  0.7361111111111112\n",
        "\n",
        "* Condition 4 - GridSearchCV Hyperparameter tuning \n",
        "\n",
        " Performance of the selected parameters:\n",
        "    confusion_matrix:\n",
        "      [[20  7  1]\n",
        "      [ 0 22  5]\n",
        "      [ 0  6 11]]\n",
        "    Accuracy Score:  0.7361111111111112\n",
        "    Precision Score:  0.7773809523809524\n",
        "    Recall Score:  0.7361111111111112\n",
        "\n",
        "By looking at the current performance of the paramters on Random Forest and MLP classifier, I can conclude that Random Forest randomized hyperparamter tuning does a better job. Gives the highest performance measure, accuracy of 98.61.\n",
        "\n",
        "Although, MLP Classifier usually does a good job, maybe using the Best parameters found:  {'activation': 'tanh', 'hidden_layer_sizes': (360, 325, 300), 'solver': 'adam'} after runing the condition -4 in MLP Classifier, further tuning gives best performing parameters for MLP classifier, which then would give a higher accuracy. But, it is a trade-off between time and accuracy, so the random forest with randomized parameter search gives a higher accuracy in a smaller/lesser time comapred to the MLP CLassifier. \n",
        "\n",
        "Therefore, I recommend, random forest model with randomizedsearchedCV paramters.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}